{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2019 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 3 - Ensamblados y modelos avanzados </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Técnicas de ensamblado: *bagging*, *boosting* y *random forest*.\n",
    "* Ventajas de técnicas de ensamblados\n",
    "* Problemas desbalanceados\n",
    " \n",
    "\n",
    "**Formalidades**  \n",
    "* Equipos de trabajo de: 2 personas (*cada uno debe estar en condiciones de realizar una presentación y discutir sobre cada punto del trabajo realizado*)\n",
    "* Se debe preparar una presentación de 20 minutos. Presentador será elegido aleatoriamente.\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega y cierre competencia: 17 de Enero\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea3-INF393-II-2019]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Ensamblados para regresión  \n",
    "[2.](#segundo) Detección de acoso en *Twitter*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Ensamblados para regresión\n",
    "---\n",
    "Las técnicas de ensamblados vistos en clases pueden ser aplicadas tanto a problemas de clasificación o regresión, teniendo la ventaja de utilizar múltiples modelos de aprendizaje para utilizar la ventaja de cada uno. En este actividad se trabajará con predecir la temperatura media de un día, dada cierta información del día anterior, como la humedad, velocidad del viento, presión atmosférica, fecha y temperatura. El modelo predictor derivado puede ser bastante útil para conocer el comportamiento del clima a lo largo del tiempo.\n",
    "\n",
    "<img src=\"https://scijinks.gov/review/forecast-reliability/forecast-reliability2.jpg\" title=\"Title text\" width=\"70%\"  />\n",
    "\n",
    "Los datos de clima son recolectados en la ciudad Delhi de India por un período de 4 años (2013 a 2017), proporcionados en Kaggle a través del siguiente __[link](https://www.kaggle.com/sumanthvrao/daily-climate-time-series-data)__, las particiones de entrenamiento y prueba están dadas. El registro de cada dato corresponde a un día, incrementando a través de las filas por cada día.\n",
    "\n",
    "---\n",
    "    \n",
    ">  Cargue los datos en un dataframe de pandas, además agregue una columna indicando el valor a predecir, la temperatura media del día siguiente. *Como el último dato/registro no tiene un valor a predecir éste se elimina*.\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"DailyDelhiClimateTrain.csv\")\n",
    "df[\"y_value\"] = df[\"meantemp\"].shift(-1)\n",
    "df = df.iloc[:-1] #remove last row\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>y_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013-01-06</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>8.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>63.714286</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>1018.714286</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2013-01-10</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>15.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2013-01-11</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>51.285714</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>1016.142857</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2013-01-12</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>13.228571</td>\n",
       "      <td>1015.571429</td>\n",
       "      <td>15.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2013-01-13</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>75.166667</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1013.333333</td>\n",
       "      <td>12.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013-01-14</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>88.166667</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1015.166667</td>\n",
       "      <td>14.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2013-01-15</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>71.857143</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>1015.857143</td>\n",
       "      <td>13.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2013-01-16</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1016.666667</td>\n",
       "      <td>16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2013-01-17</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>80.833333</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1015.833333</td>\n",
       "      <td>13.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>92.166667</td>\n",
       "      <td>8.950000</td>\n",
       "      <td>1014.500000</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2013-01-19</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>76.666667</td>\n",
       "      <td>5.883333</td>\n",
       "      <td>1021.666667</td>\n",
       "      <td>11.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2013-01-20</td>\n",
       "      <td>11.285714</td>\n",
       "      <td>75.285714</td>\n",
       "      <td>8.471429</td>\n",
       "      <td>1020.285714</td>\n",
       "      <td>11.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2013-01-21</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2013-01-22</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>79.666667</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>1021.800000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2013-01-23</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>60.166667</td>\n",
       "      <td>4.016667</td>\n",
       "      <td>1020.500000</td>\n",
       "      <td>13.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>60.666667</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>1020.500000</td>\n",
       "      <td>12.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2013-01-25</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>1020.750000</td>\n",
       "      <td>12.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2013-01-26</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>64.166667</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>1019.666667</td>\n",
       "      <td>12.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2013-01-27</td>\n",
       "      <td>12.857143</td>\n",
       "      <td>65.571429</td>\n",
       "      <td>5.557143</td>\n",
       "      <td>1018.142857</td>\n",
       "      <td>14.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2013-01-28</td>\n",
       "      <td>14.833333</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1017.833333</td>\n",
       "      <td>14.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2013-01-29</td>\n",
       "      <td>14.125000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>3.237500</td>\n",
       "      <td>1016.625000</td>\n",
       "      <td>14.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2013-01-30</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>70.428571</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>1017.857143</td>\n",
       "      <td>16.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>19.208333</td>\n",
       "      <td>75.875000</td>\n",
       "      <td>4.945833</td>\n",
       "      <td>1017.750000</td>\n",
       "      <td>21.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2016-12-03</td>\n",
       "      <td>21.208333</td>\n",
       "      <td>52.166667</td>\n",
       "      <td>5.866667</td>\n",
       "      <td>1019.333333</td>\n",
       "      <td>18.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>55.250000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>1019.700000</td>\n",
       "      <td>18.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>18.636364</td>\n",
       "      <td>56.590909</td>\n",
       "      <td>4.952381</td>\n",
       "      <td>1017.045455</td>\n",
       "      <td>18.538462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>18.538462</td>\n",
       "      <td>69.923077</td>\n",
       "      <td>2.503846</td>\n",
       "      <td>1017.961538</td>\n",
       "      <td>18.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2016-12-07</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>74.350000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>1017.421053</td>\n",
       "      <td>16.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2016-12-08</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>73.300000</td>\n",
       "      <td>1.765000</td>\n",
       "      <td>1016.200000</td>\n",
       "      <td>19.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2016-12-09</td>\n",
       "      <td>19.416667</td>\n",
       "      <td>68.125000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1013.416667</td>\n",
       "      <td>16.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2016-12-10</td>\n",
       "      <td>16.444444</td>\n",
       "      <td>82.833333</td>\n",
       "      <td>5.355556</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>20.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2016-12-11</td>\n",
       "      <td>20.041667</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>4.716667</td>\n",
       "      <td>1013.291667</td>\n",
       "      <td>19.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2016-12-12</td>\n",
       "      <td>19.909091</td>\n",
       "      <td>63.863636</td>\n",
       "      <td>3.281818</td>\n",
       "      <td>1014.181818</td>\n",
       "      <td>19.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2016-12-13</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>62.350000</td>\n",
       "      <td>3.430000</td>\n",
       "      <td>1015.100000</td>\n",
       "      <td>18.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>2016-12-14</td>\n",
       "      <td>18.555556</td>\n",
       "      <td>58.611111</td>\n",
       "      <td>8.027778</td>\n",
       "      <td>1017.333333</td>\n",
       "      <td>18.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>2016-12-15</td>\n",
       "      <td>18.166667</td>\n",
       "      <td>56.625000</td>\n",
       "      <td>9.879167</td>\n",
       "      <td>1016.666667</td>\n",
       "      <td>15.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>63.277778</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>1018.777778</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>2016-12-17</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>63.388889</td>\n",
       "      <td>6.731579</td>\n",
       "      <td>1016.947368</td>\n",
       "      <td>16.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>16.083333</td>\n",
       "      <td>64.541667</td>\n",
       "      <td>6.420833</td>\n",
       "      <td>1018.083333</td>\n",
       "      <td>17.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>17.857143</td>\n",
       "      <td>56.095238</td>\n",
       "      <td>10.414286</td>\n",
       "      <td>1017.428571</td>\n",
       "      <td>19.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>2016-12-20</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>48.533333</td>\n",
       "      <td>15.926667</td>\n",
       "      <td>1015.200000</td>\n",
       "      <td>18.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>2016-12-21</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>54.300000</td>\n",
       "      <td>19.404762</td>\n",
       "      <td>1015.619048</td>\n",
       "      <td>17.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>17.285714</td>\n",
       "      <td>57.857143</td>\n",
       "      <td>6.180952</td>\n",
       "      <td>1016.142857</td>\n",
       "      <td>15.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>15.550000</td>\n",
       "      <td>74.700000</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>1014.250000</td>\n",
       "      <td>17.318182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>2016-12-24</td>\n",
       "      <td>17.318182</td>\n",
       "      <td>78.636364</td>\n",
       "      <td>5.236364</td>\n",
       "      <td>1011.318182</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2016-12-25</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>94.300000</td>\n",
       "      <td>9.085000</td>\n",
       "      <td>1014.350000</td>\n",
       "      <td>17.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2016-12-26</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>74.857143</td>\n",
       "      <td>8.784211</td>\n",
       "      <td>1016.952381</td>\n",
       "      <td>16.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2016-12-27</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>67.550000</td>\n",
       "      <td>8.335000</td>\n",
       "      <td>1017.200000</td>\n",
       "      <td>17.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2016-12-28</td>\n",
       "      <td>17.217391</td>\n",
       "      <td>68.043478</td>\n",
       "      <td>3.547826</td>\n",
       "      <td>1015.565217</td>\n",
       "      <td>15.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2016-12-29</td>\n",
       "      <td>15.238095</td>\n",
       "      <td>87.857143</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1016.904762</td>\n",
       "      <td>14.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>14.095238</td>\n",
       "      <td>89.666667</td>\n",
       "      <td>6.266667</td>\n",
       "      <td>1017.904762</td>\n",
       "      <td>15.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>15.052632</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.325000</td>\n",
       "      <td>1016.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1461 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date   meantemp   humidity  wind_speed  meanpressure    y_value\n",
       "0     2013-01-01  10.000000  84.500000    0.000000   1015.666667   7.400000\n",
       "1     2013-01-02   7.400000  92.000000    2.980000   1017.800000   7.166667\n",
       "2     2013-01-03   7.166667  87.000000    4.633333   1018.666667   8.666667\n",
       "3     2013-01-04   8.666667  71.333333    1.233333   1017.166667   6.000000\n",
       "4     2013-01-05   6.000000  86.833333    3.700000   1016.500000   7.000000\n",
       "5     2013-01-06   7.000000  82.800000    1.480000   1018.000000   7.000000\n",
       "6     2013-01-07   7.000000  78.600000    6.300000   1020.000000   8.857143\n",
       "7     2013-01-08   8.857143  63.714286    7.142857   1018.714286  14.000000\n",
       "8     2013-01-09  14.000000  51.250000   12.500000   1017.000000  11.000000\n",
       "9     2013-01-10  11.000000  62.000000    7.400000   1015.666667  15.714286\n",
       "10    2013-01-11  15.714286  51.285714   10.571429   1016.142857  14.000000\n",
       "11    2013-01-12  14.000000  74.000000   13.228571   1015.571429  15.833333\n",
       "12    2013-01-13  15.833333  75.166667    4.633333   1013.333333  12.833333\n",
       "13    2013-01-14  12.833333  88.166667    0.616667   1015.166667  14.714286\n",
       "14    2013-01-15  14.714286  71.857143    0.528571   1015.857143  13.833333\n",
       "15    2013-01-16  13.833333  86.666667    0.000000   1016.666667  16.500000\n",
       "16    2013-01-17  16.500000  80.833333    5.250000   1015.833333  13.833333\n",
       "17    2013-01-18  13.833333  92.166667    8.950000   1014.500000  12.500000\n",
       "18    2013-01-19  12.500000  76.666667    5.883333   1021.666667  11.285714\n",
       "19    2013-01-20  11.285714  75.285714    8.471429   1020.285714  11.200000\n",
       "20    2013-01-21  11.200000  77.000000    2.220000   1021.000000   9.500000\n",
       "21    2013-01-22   9.500000  79.666667    3.083333   1021.800000  14.000000\n",
       "22    2013-01-23  14.000000  60.166667    4.016667   1020.500000  13.833333\n",
       "23    2013-01-24  13.833333  60.666667    6.166667   1020.500000  12.250000\n",
       "24    2013-01-25  12.250000  67.000000    5.550000   1020.750000  12.666667\n",
       "25    2013-01-26  12.666667  64.166667    6.800000   1019.666667  12.857143\n",
       "26    2013-01-27  12.857143  65.571429    5.557143   1018.142857  14.833333\n",
       "27    2013-01-28  14.833333  56.000000    3.700000   1017.833333  14.125000\n",
       "28    2013-01-29  14.125000  65.500000    3.237500   1016.625000  14.714286\n",
       "29    2013-01-30  14.714286  70.428571    1.057143   1017.857143  16.200000\n",
       "...          ...        ...        ...         ...           ...        ...\n",
       "1431  2016-12-02  19.208333  75.875000    4.945833   1017.750000  21.208333\n",
       "1432  2016-12-03  21.208333  52.166667    5.866667   1019.333333  18.900000\n",
       "1433  2016-12-04  18.900000  55.250000    5.666667   1019.700000  18.636364\n",
       "1434  2016-12-05  18.636364  56.590909    4.952381   1017.045455  18.538462\n",
       "1435  2016-12-06  18.538462  69.923077    2.503846   1017.961538  18.250000\n",
       "1436  2016-12-07  18.250000  74.350000    0.925000   1017.421053  16.900000\n",
       "1437  2016-12-08  16.900000  73.300000    1.765000   1016.200000  19.416667\n",
       "1438  2016-12-09  19.416667  68.125000    1.312500   1013.416667  16.444444\n",
       "1439  2016-12-10  16.444444  82.833333    5.355556   1014.000000  20.041667\n",
       "1440  2016-12-11  20.041667  69.583333    4.716667   1013.291667  19.909091\n",
       "1441  2016-12-12  19.909091  63.863636    3.281818   1014.181818  19.050000\n",
       "1442  2016-12-13  19.050000  62.350000    3.430000   1015.100000  18.555556\n",
       "1443  2016-12-14  18.555556  58.611111    8.027778   1017.333333  18.166667\n",
       "1444  2016-12-15  18.166667  56.625000    9.879167   1016.666667  15.833333\n",
       "1445  2016-12-16  15.833333  63.277778    3.916667   1018.777778  17.500000\n",
       "1446  2016-12-17  17.500000  63.388889    6.731579   1016.947368  16.083333\n",
       "1447  2016-12-18  16.083333  64.541667    6.420833   1018.083333  17.857143\n",
       "1448  2016-12-19  17.857143  56.095238   10.414286   1017.428571  19.800000\n",
       "1449  2016-12-20  19.800000  48.533333   15.926667   1015.200000  18.050000\n",
       "1450  2016-12-21  18.050000  54.300000   19.404762   1015.619048  17.285714\n",
       "1451  2016-12-22  17.285714  57.857143    6.180952   1016.142857  15.550000\n",
       "1452  2016-12-23  15.550000  74.700000    1.205000   1014.250000  17.318182\n",
       "1453  2016-12-24  17.318182  78.636364    5.236364   1011.318182  14.000000\n",
       "1454  2016-12-25  14.000000  94.300000    9.085000   1014.350000  17.142857\n",
       "1455  2016-12-26  17.142857  74.857143    8.784211   1016.952381  16.850000\n",
       "1456  2016-12-27  16.850000  67.550000    8.335000   1017.200000  17.217391\n",
       "1457  2016-12-28  17.217391  68.043478    3.547826   1015.565217  15.238095\n",
       "1458  2016-12-29  15.238095  87.857143    6.000000   1016.904762  14.095238\n",
       "1459  2016-12-30  14.095238  89.666667    6.266667   1017.904762  15.052632\n",
       "1460  2016-12-31  15.052632  87.000000    7.325000   1016.100000  10.000000\n",
       "\n",
       "[1461 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"DailyDelhiClimateTrain.csv\")\n",
    "df[\"y_value\"] = df[\"meantemp\"].shift(-1)\n",
    "df = df.iloc[:-1] #remove last row\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Debido a la poca información que se tiene a través de los 4 parámetros medidos, extraíga más información a través de los datos de fecha. Por ejemplo, el comportamiento a través de los meses y años varía, así como la información de la temporada del año podría ayudar a la predicción. Decida si puede incluir más información a partir de la fecha que tenga sentido con el problema.\n",
    "```python\n",
    "...#procesamiento de fecha(datetime/timestamp) a numeros\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df['cday'] = df['date'].dt.dayofweek #0:lunes,6:domingo\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month #1:enero, 12: diciembre\n",
    "...#based on: https://en.wikipedia.org/wiki/Climate_of_India\n",
    "seasons = [\"winter\",\"winter\",\"summer\",\"summer\",\"summer\",\"rainy\",\"rainy\",\"rainy\",\"fall\",\"fall\",\"fall\",\"winter\"]\n",
    "df['season'] = [ seasons[month_i - 1] for month_i in df['month'].values ]\n",
    "df = pd.get_dummies(df,columns=['season']) #to one hot.. as nominal variable\n",
    "... #any more information?\n",
    "df.drop([\"date\"], axis=1, inplace=True) #delete date\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>y_value</th>\n",
       "      <th>cday</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>season_fall</th>\n",
       "      <th>season_rainy</th>\n",
       "      <th>season_summer</th>\n",
       "      <th>season_winter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.857143</td>\n",
       "      <td>63.714286</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>1018.714286</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.714286</td>\n",
       "      <td>51.285714</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>1016.142857</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>13.228571</td>\n",
       "      <td>1015.571429</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15.833333</td>\n",
       "      <td>75.166667</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1013.333333</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12.833333</td>\n",
       "      <td>88.166667</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1015.166667</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.714286</td>\n",
       "      <td>71.857143</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>1015.857143</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.833333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1016.666667</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.500000</td>\n",
       "      <td>80.833333</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1015.833333</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.833333</td>\n",
       "      <td>92.166667</td>\n",
       "      <td>8.950000</td>\n",
       "      <td>1014.500000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>12.500000</td>\n",
       "      <td>76.666667</td>\n",
       "      <td>5.883333</td>\n",
       "      <td>1021.666667</td>\n",
       "      <td>11.285714</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.285714</td>\n",
       "      <td>75.285714</td>\n",
       "      <td>8.471429</td>\n",
       "      <td>1020.285714</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>6</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11.200000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>2.220000</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9.500000</td>\n",
       "      <td>79.666667</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>1021.800000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>60.166667</td>\n",
       "      <td>4.016667</td>\n",
       "      <td>1020.500000</td>\n",
       "      <td>13.833333</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13.833333</td>\n",
       "      <td>60.666667</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>1020.500000</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12.250000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>5.550000</td>\n",
       "      <td>1020.750000</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12.666667</td>\n",
       "      <td>64.166667</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>1019.666667</td>\n",
       "      <td>12.857143</td>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>12.857143</td>\n",
       "      <td>65.571429</td>\n",
       "      <td>5.557143</td>\n",
       "      <td>1018.142857</td>\n",
       "      <td>14.833333</td>\n",
       "      <td>6</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14.833333</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1017.833333</td>\n",
       "      <td>14.125000</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14.125000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>3.237500</td>\n",
       "      <td>1016.625000</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>14.714286</td>\n",
       "      <td>70.428571</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>1017.857143</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>19.208333</td>\n",
       "      <td>75.875000</td>\n",
       "      <td>4.945833</td>\n",
       "      <td>1017.750000</td>\n",
       "      <td>21.208333</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>21.208333</td>\n",
       "      <td>52.166667</td>\n",
       "      <td>5.866667</td>\n",
       "      <td>1019.333333</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>18.900000</td>\n",
       "      <td>55.250000</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>1019.700000</td>\n",
       "      <td>18.636364</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>18.636364</td>\n",
       "      <td>56.590909</td>\n",
       "      <td>4.952381</td>\n",
       "      <td>1017.045455</td>\n",
       "      <td>18.538462</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>18.538462</td>\n",
       "      <td>69.923077</td>\n",
       "      <td>2.503846</td>\n",
       "      <td>1017.961538</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>18.250000</td>\n",
       "      <td>74.350000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>1017.421053</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>16.900000</td>\n",
       "      <td>73.300000</td>\n",
       "      <td>1.765000</td>\n",
       "      <td>1016.200000</td>\n",
       "      <td>19.416667</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>19.416667</td>\n",
       "      <td>68.125000</td>\n",
       "      <td>1.312500</td>\n",
       "      <td>1013.416667</td>\n",
       "      <td>16.444444</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>16.444444</td>\n",
       "      <td>82.833333</td>\n",
       "      <td>5.355556</td>\n",
       "      <td>1014.000000</td>\n",
       "      <td>20.041667</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>20.041667</td>\n",
       "      <td>69.583333</td>\n",
       "      <td>4.716667</td>\n",
       "      <td>1013.291667</td>\n",
       "      <td>19.909091</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>19.909091</td>\n",
       "      <td>63.863636</td>\n",
       "      <td>3.281818</td>\n",
       "      <td>1014.181818</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>19.050000</td>\n",
       "      <td>62.350000</td>\n",
       "      <td>3.430000</td>\n",
       "      <td>1015.100000</td>\n",
       "      <td>18.555556</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>18.555556</td>\n",
       "      <td>58.611111</td>\n",
       "      <td>8.027778</td>\n",
       "      <td>1017.333333</td>\n",
       "      <td>18.166667</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>18.166667</td>\n",
       "      <td>56.625000</td>\n",
       "      <td>9.879167</td>\n",
       "      <td>1016.666667</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>15.833333</td>\n",
       "      <td>63.277778</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>1018.777778</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>17.500000</td>\n",
       "      <td>63.388889</td>\n",
       "      <td>6.731579</td>\n",
       "      <td>1016.947368</td>\n",
       "      <td>16.083333</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>16.083333</td>\n",
       "      <td>64.541667</td>\n",
       "      <td>6.420833</td>\n",
       "      <td>1018.083333</td>\n",
       "      <td>17.857143</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>17.857143</td>\n",
       "      <td>56.095238</td>\n",
       "      <td>10.414286</td>\n",
       "      <td>1017.428571</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>19.800000</td>\n",
       "      <td>48.533333</td>\n",
       "      <td>15.926667</td>\n",
       "      <td>1015.200000</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>18.050000</td>\n",
       "      <td>54.300000</td>\n",
       "      <td>19.404762</td>\n",
       "      <td>1015.619048</td>\n",
       "      <td>17.285714</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>17.285714</td>\n",
       "      <td>57.857143</td>\n",
       "      <td>6.180952</td>\n",
       "      <td>1016.142857</td>\n",
       "      <td>15.550000</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>15.550000</td>\n",
       "      <td>74.700000</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>1014.250000</td>\n",
       "      <td>17.318182</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>17.318182</td>\n",
       "      <td>78.636364</td>\n",
       "      <td>5.236364</td>\n",
       "      <td>1011.318182</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>94.300000</td>\n",
       "      <td>9.085000</td>\n",
       "      <td>1014.350000</td>\n",
       "      <td>17.142857</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>17.142857</td>\n",
       "      <td>74.857143</td>\n",
       "      <td>8.784211</td>\n",
       "      <td>1016.952381</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>16.850000</td>\n",
       "      <td>67.550000</td>\n",
       "      <td>8.335000</td>\n",
       "      <td>1017.200000</td>\n",
       "      <td>17.217391</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>17.217391</td>\n",
       "      <td>68.043478</td>\n",
       "      <td>3.547826</td>\n",
       "      <td>1015.565217</td>\n",
       "      <td>15.238095</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>15.238095</td>\n",
       "      <td>87.857143</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1016.904762</td>\n",
       "      <td>14.095238</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>14.095238</td>\n",
       "      <td>89.666667</td>\n",
       "      <td>6.266667</td>\n",
       "      <td>1017.904762</td>\n",
       "      <td>15.052632</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>15.052632</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>7.325000</td>\n",
       "      <td>1016.100000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1461 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       meantemp   humidity  wind_speed  meanpressure    y_value  cday  year  \\\n",
       "0     10.000000  84.500000    0.000000   1015.666667   7.400000     1  2013   \n",
       "1      7.400000  92.000000    2.980000   1017.800000   7.166667     2  2013   \n",
       "2      7.166667  87.000000    4.633333   1018.666667   8.666667     3  2013   \n",
       "3      8.666667  71.333333    1.233333   1017.166667   6.000000     4  2013   \n",
       "4      6.000000  86.833333    3.700000   1016.500000   7.000000     5  2013   \n",
       "5      7.000000  82.800000    1.480000   1018.000000   7.000000     6  2013   \n",
       "6      7.000000  78.600000    6.300000   1020.000000   8.857143     0  2013   \n",
       "7      8.857143  63.714286    7.142857   1018.714286  14.000000     1  2013   \n",
       "8     14.000000  51.250000   12.500000   1017.000000  11.000000     2  2013   \n",
       "9     11.000000  62.000000    7.400000   1015.666667  15.714286     3  2013   \n",
       "10    15.714286  51.285714   10.571429   1016.142857  14.000000     4  2013   \n",
       "11    14.000000  74.000000   13.228571   1015.571429  15.833333     5  2013   \n",
       "12    15.833333  75.166667    4.633333   1013.333333  12.833333     6  2013   \n",
       "13    12.833333  88.166667    0.616667   1015.166667  14.714286     0  2013   \n",
       "14    14.714286  71.857143    0.528571   1015.857143  13.833333     1  2013   \n",
       "15    13.833333  86.666667    0.000000   1016.666667  16.500000     2  2013   \n",
       "16    16.500000  80.833333    5.250000   1015.833333  13.833333     3  2013   \n",
       "17    13.833333  92.166667    8.950000   1014.500000  12.500000     4  2013   \n",
       "18    12.500000  76.666667    5.883333   1021.666667  11.285714     5  2013   \n",
       "19    11.285714  75.285714    8.471429   1020.285714  11.200000     6  2013   \n",
       "20    11.200000  77.000000    2.220000   1021.000000   9.500000     0  2013   \n",
       "21     9.500000  79.666667    3.083333   1021.800000  14.000000     1  2013   \n",
       "22    14.000000  60.166667    4.016667   1020.500000  13.833333     2  2013   \n",
       "23    13.833333  60.666667    6.166667   1020.500000  12.250000     3  2013   \n",
       "24    12.250000  67.000000    5.550000   1020.750000  12.666667     4  2013   \n",
       "25    12.666667  64.166667    6.800000   1019.666667  12.857143     5  2013   \n",
       "26    12.857143  65.571429    5.557143   1018.142857  14.833333     6  2013   \n",
       "27    14.833333  56.000000    3.700000   1017.833333  14.125000     0  2013   \n",
       "28    14.125000  65.500000    3.237500   1016.625000  14.714286     1  2013   \n",
       "29    14.714286  70.428571    1.057143   1017.857143  16.200000     2  2013   \n",
       "...         ...        ...         ...           ...        ...   ...   ...   \n",
       "1431  19.208333  75.875000    4.945833   1017.750000  21.208333     4  2016   \n",
       "1432  21.208333  52.166667    5.866667   1019.333333  18.900000     5  2016   \n",
       "1433  18.900000  55.250000    5.666667   1019.700000  18.636364     6  2016   \n",
       "1434  18.636364  56.590909    4.952381   1017.045455  18.538462     0  2016   \n",
       "1435  18.538462  69.923077    2.503846   1017.961538  18.250000     1  2016   \n",
       "1436  18.250000  74.350000    0.925000   1017.421053  16.900000     2  2016   \n",
       "1437  16.900000  73.300000    1.765000   1016.200000  19.416667     3  2016   \n",
       "1438  19.416667  68.125000    1.312500   1013.416667  16.444444     4  2016   \n",
       "1439  16.444444  82.833333    5.355556   1014.000000  20.041667     5  2016   \n",
       "1440  20.041667  69.583333    4.716667   1013.291667  19.909091     6  2016   \n",
       "1441  19.909091  63.863636    3.281818   1014.181818  19.050000     0  2016   \n",
       "1442  19.050000  62.350000    3.430000   1015.100000  18.555556     1  2016   \n",
       "1443  18.555556  58.611111    8.027778   1017.333333  18.166667     2  2016   \n",
       "1444  18.166667  56.625000    9.879167   1016.666667  15.833333     3  2016   \n",
       "1445  15.833333  63.277778    3.916667   1018.777778  17.500000     4  2016   \n",
       "1446  17.500000  63.388889    6.731579   1016.947368  16.083333     5  2016   \n",
       "1447  16.083333  64.541667    6.420833   1018.083333  17.857143     6  2016   \n",
       "1448  17.857143  56.095238   10.414286   1017.428571  19.800000     0  2016   \n",
       "1449  19.800000  48.533333   15.926667   1015.200000  18.050000     1  2016   \n",
       "1450  18.050000  54.300000   19.404762   1015.619048  17.285714     2  2016   \n",
       "1451  17.285714  57.857143    6.180952   1016.142857  15.550000     3  2016   \n",
       "1452  15.550000  74.700000    1.205000   1014.250000  17.318182     4  2016   \n",
       "1453  17.318182  78.636364    5.236364   1011.318182  14.000000     5  2016   \n",
       "1454  14.000000  94.300000    9.085000   1014.350000  17.142857     6  2016   \n",
       "1455  17.142857  74.857143    8.784211   1016.952381  16.850000     0  2016   \n",
       "1456  16.850000  67.550000    8.335000   1017.200000  17.217391     1  2016   \n",
       "1457  17.217391  68.043478    3.547826   1015.565217  15.238095     2  2016   \n",
       "1458  15.238095  87.857143    6.000000   1016.904762  14.095238     3  2016   \n",
       "1459  14.095238  89.666667    6.266667   1017.904762  15.052632     4  2016   \n",
       "1460  15.052632  87.000000    7.325000   1016.100000  10.000000     5  2016   \n",
       "\n",
       "      month  season_fall  season_rainy  season_summer  season_winter  \n",
       "0         1            0             0              0              1  \n",
       "1         1            0             0              0              1  \n",
       "2         1            0             0              0              1  \n",
       "3         1            0             0              0              1  \n",
       "4         1            0             0              0              1  \n",
       "5         1            0             0              0              1  \n",
       "6         1            0             0              0              1  \n",
       "7         1            0             0              0              1  \n",
       "8         1            0             0              0              1  \n",
       "9         1            0             0              0              1  \n",
       "10        1            0             0              0              1  \n",
       "11        1            0             0              0              1  \n",
       "12        1            0             0              0              1  \n",
       "13        1            0             0              0              1  \n",
       "14        1            0             0              0              1  \n",
       "15        1            0             0              0              1  \n",
       "16        1            0             0              0              1  \n",
       "17        1            0             0              0              1  \n",
       "18        1            0             0              0              1  \n",
       "19        1            0             0              0              1  \n",
       "20        1            0             0              0              1  \n",
       "21        1            0             0              0              1  \n",
       "22        1            0             0              0              1  \n",
       "23        1            0             0              0              1  \n",
       "24        1            0             0              0              1  \n",
       "25        1            0             0              0              1  \n",
       "26        1            0             0              0              1  \n",
       "27        1            0             0              0              1  \n",
       "28        1            0             0              0              1  \n",
       "29        1            0             0              0              1  \n",
       "...     ...          ...           ...            ...            ...  \n",
       "1431     12            0             0              0              1  \n",
       "1432     12            0             0              0              1  \n",
       "1433     12            0             0              0              1  \n",
       "1434     12            0             0              0              1  \n",
       "1435     12            0             0              0              1  \n",
       "1436     12            0             0              0              1  \n",
       "1437     12            0             0              0              1  \n",
       "1438     12            0             0              0              1  \n",
       "1439     12            0             0              0              1  \n",
       "1440     12            0             0              0              1  \n",
       "1441     12            0             0              0              1  \n",
       "1442     12            0             0              0              1  \n",
       "1443     12            0             0              0              1  \n",
       "1444     12            0             0              0              1  \n",
       "1445     12            0             0              0              1  \n",
       "1446     12            0             0              0              1  \n",
       "1447     12            0             0              0              1  \n",
       "1448     12            0             0              0              1  \n",
       "1449     12            0             0              0              1  \n",
       "1450     12            0             0              0              1  \n",
       "1451     12            0             0              0              1  \n",
       "1452     12            0             0              0              1  \n",
       "1453     12            0             0              0              1  \n",
       "1454     12            0             0              0              1  \n",
       "1455     12            0             0              0              1  \n",
       "1456     12            0             0              0              1  \n",
       "1457     12            0             0              0              1  \n",
       "1458     12            0             0              0              1  \n",
       "1459     12            0             0              0              1  \n",
       "1460     12            0             0              0              1  \n",
       "\n",
       "[1461 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#procesamiento de fecha(datetime/timestamp) a numeros\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df['cday'] = df['date'].dt.dayofweek #0:lunes,6:domingo\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month #1:enero, 12: diciembre\n",
    "#based on: https://en.wikipedia.org/wiki/Climate_of_India\n",
    "seasons = [\"winter\",\"winter\",\"summer\",\"summer\",\"summer\",\"rainy\",\"rainy\",\"rainy\",\"fall\",\"fall\",\"fall\",\"winter\"]\n",
    "df['season'] = [ seasons[month_i - 1] for month_i in df['month'].values ]\n",
    "df = pd.get_dummies(df,columns=['season']) #to one hot.. as nominal variable\n",
    "#any more information?\n",
    "df.drop([\"date\"], axis=1, inplace=True) #delete date\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cree las matrices de entrenamiento, con los mil primeros registros, y de validación, con el resto. Para evitar el orden natural en que vienen los datos entrenados, realice un *shuffle* aleatorio.\n",
    "```python\n",
    "y = df.pop(\"y_value\").values\n",
    "X = df.values \n",
    "X_train = X[:1000]\n",
    "y_train = y[:1000]\n",
    "X_val = X[1000:]\n",
    "y_val = y[1000:]\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0) #shuffle values on train only\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop(\"y_value\").values\n",
    "X = df.values \n",
    "X_train = X[:1000]\n",
    "y_train = y[:1000]\n",
    "X_val = X[1000:]\n",
    "y_val = y[1000:]\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state=0) #shuffle values on train only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a) Describa el problema trabajado, la cantida de datos que se cuenta como las características a trabajar. Al ser datos temporales podría ayudar una ilustración gráfica de la secuencias trabajadas y su comportamiento ¿Es válido el uso de la información sólo del día anterior?.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Entrene un solo Árbol de Regresión de múltiples niveles para resolver el problema. Defina un Árbol **no regularizado** (como el que no tiene límites en su profundidad) y otro Árbol **regularizado** (variando los hiper-parámetros que prefiera, por ejemplo, los más comunes como la profundidad, el número mínimo de datos para realizar *split* o el número mínimo de datos en cada hoja). Además comente sobre la ventaja de usar un árbol de decisión respecto a la escala de los datos ¿Porqué no es necesario escalar los datos?\n",
    "```python\n",
    "import numpy as np\n",
    "def RMSE(ytrue,ypred):\n",
    "    return np.sqrt(np.mean(np.square(ytrue - ypred)) )\n",
    "from sklearn.tree import DecisionTreeRegressor as Tree\n",
    "model_unr = Tree() #unregularized model -- default parameters\n",
    "model_unr.fit(X_train,y_train)\n",
    "... #define your regularized tree model\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8903983696050223"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def RMSE(ytrue,ypred):\n",
    "    return np.sqrt(np.mean(np.square(ytrue - ypred)) )\n",
    "from sklearn.tree import DecisionTreeRegressor as Tree\n",
    "model_unr = Tree() #unregularized model -- default parameters\n",
    "model_unr.fit(X_train,y_train)\n",
    "model_unr.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> c) Para evaluar la calidad de predicción en este problema se utilizará la métrica *Root Mean Squared Error* (RMSE), indicando un error en la escala real de la temperatura. Como los datos de validación siguen con el orden temporal, visualice esa predicción a lo largo del tiempo. Comente sobre los resultados comparando la regularización *vs* el no regularizar.\n",
    "```python\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_val_hat = model.predict(X_val)\n",
    "print(\"RMSE train= \",RMSE(y_train,y_train_hat))\n",
    "print(\"RMSE val= \",RMSE(y_val,y_val_hat))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(y_val, '.-' ,label=\"True values\")\n",
    "plt.plot(y_val_hat, '.-' ,label=\"Pred values\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE train=  0.0\n",
      "RMSE val=  2.3456541587455075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_hat = model_unr.predict(X_train)\n",
    "y_val_hat = model_unr.predict(X_val)\n",
    "print(\"RMSE train= \",RMSE(y_train,y_train_hat))\n",
    "print(\"RMSE val= \",RMSE(y_val,y_val_hat))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(y_val, '.-' ,label=\"True values\")\n",
    "plt.plot(y_val_hat, '.-' ,label=\"Pred values\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d) Entrene un ensamblado de árboles de múltiples niveles, mediante la técnica de **Bagging**, compare el Árbol **no regularizado** con el **regularizado** (*seteando los hiper-parámetros en base a lo experimentado anteriormente en b)*) ¿Qué debería suceder? ¿Se visualiza *overfitting*? Varíe la cantidad de árboles de decisión utilizados en el ensamblado (*n estimators*), realice un gráfico resumen del RMSE de entrenamiento y validación en función de este hiper-parámetro.\n",
    "```python\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "model = BaggingRegressor(base_estimator=Tree(...), n_estimators=..., n_jobs=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Entrene un ensamblado de árboles de múltiples niveles, mediante la técnica de **AdaBoost**, compare el Árbol **no regularizado** con el **regularizado** (*seteando los hiper-parámetros en base a lo experimentado anteriormente en d)* ¿Se visualiza *overfitting*? ¿Qué técnica utiliza la librería de sklearn, *re-muestrear* o *pesar* ejemplos? ¿Qué le parece más sensato?. Varíe la cantidad de árboles de decisión utilizados en el ensamblado (*n estimators*), realice un gráfico resumen del RMSE de entrenamiento y validación en función de este hiper-parámetro. Compare y analice con la técnica utilizada en d).\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model = AdaBoostRegressor(base_estimator=Tree(...), n_estimators=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> f) Pruebe otra técnica de ensamblado dedicada a árboles de decisión, que combina el muestreo *boostrap* de *Bagging* con muestreo sobre las *features*: **Random Forest**, compare el Árbol **no regularizado** con el **regularizado** ¿Se visualiza *overfitting*?. Varíe la cantidad de árboles de decisión utilizados en el ensamblado (*n estimators*), realice un gráfico resumen del RMSE de entrenamiento y validación en función de este hiper-parámetro.\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_unr = RandomForestRegressor(n_estimators=..., n_jobs=-1)\n",
    "... #define your regularized random forest model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> g) Verifique que el **OOB error** (*out of bag error*) de los ensambladores que utilizan la técnica *boostrap* puede ser una alternativa como métrica de generalización, compare con el error calculado sobre el conjunto de validación (o en su defecto *cross validation*).\n",
    "```python\n",
    "oob_error = model.oob_score_\n",
    "val_error = model.score(X_val,y_val)\n",
    "print(\"OOB error: \",oob_error)\n",
    "print (\"Val error: \",val_error)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> h) Defina otra forma de combinar los valores que entregan los ensamblados al hacer predicciones y compare con lo que se hace actualmente, por ejemplo *Bagging* realiza el voto de la mayoría para clasificación y promedio para regresión, *AdaBoost* realiza una combinación ponderada de cada clasificador dependiendo de su *habilidad* (desempeño para clasificar el conjunto de entrenamiento). Se puede inspirar desde clásicos estadísticos, como entregar el primer cuartíl ($Q_1$) si al ensamblado le cuesta predecir valores bajos, o el segundo cuartil ($Q_2$) o mediana para ser robusto a predicciones atípicas de modelos.  \n",
    "```python\n",
    "def combine_predictions(predictions):\n",
    "    return #define !\n",
    "list_estimators = model.estimators_\n",
    "list_predictions = [estimator.predict(X_val) for estimator in list_estimators]\n",
    "new_predictions = combine_predictions(list_predictions)\n",
    "print(\"RMSE val= \",RMSE(y_val, new_predictions))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i) Si se cuenta con una gran cantidad de modelos en el ensamblado, por ejemplo $T>100$, se puede crear un intervalo de confianza de la predicción a través de todos estos valores, asumiendo una distribución Normal centrada en la media muestral de las predicciones, con desviación estándar muestral en las predicciones. El intervalo de confianza entrega más información que un único valor puntual de predicción. Visualice un intervalo de confianza al 95% de probabilidad en la predicción a lo largo de la serie de tiempo de validación, comente. Al asumir una distribución Normal, también puede explorar el tomar como predicción del ensamblado el muestreo sobre la distribución Normal creada entorno a los datos muestrales.\n",
    "```python\n",
    "X_val_est = np.vstack(list_predictions).T #has shape=(N_test, n_estimator), with n_estimator>100\n",
    "from scipy.stats import norm\n",
    "interv_val = []\n",
    "for n in range(X_val.shape[0]):\n",
    "    low, up = norm.interval(0.95, loc=np.mean(X_val_est[n]), scale=np.std(X_val_est[n]))\n",
    "    interv_val.append([low,up])\n",
    "interv_val = np.asarray(interv_val)\n",
    "x = np.arange(X_val_est.shape[0])\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x, np.mean(X_val_est, axis=1))\n",
    "plt.fill_between(x, interv_val[:,0], interv_val[:,1], color='r', alpha=.55)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  j) Evalúe y visualice la predicción del mejor modelo encontrado para resolver este problema, en el conjunto de pruebas. Además, compare y analice las distintas maneras con las que se resolvió el problema, incluya las decisiones que conlleva y los resultados que reflejan.\n",
    "```python\n",
    "df = pd.read_csv(\"DailyDelhiClimateTest.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"segundo\"></a>\n",
    "## 2. Detección de acoso en *Twitter*\n",
    "---\n",
    "En las redes sociales muchas veces se encuentra con un cierto comportamiento indeseable para los usuarios, tal como racismo, misógeno, grupos de odio o *trolls*. El poder detectar de manera automática ciertos patrones en el comportamiento para tomar una acción debe ser crucial para reducir el tiempo y esfuerzo humano. En esta actividad se trabajará sobre *tweets* la red social de *twitter* para detectar comportamiento *online* de acoso (*harassment*), que por lo general, incluye *flaming* como lenguaje abusivo o insultos, *doxing* como mostrar la información personal de una mujer, por ejemplo el domicilio o número de teléfono, la suplantación o la vergüenza pública por destruir la reputación de las personas.\n",
    "\n",
    "<img src=\"https://kidshelpline.com.au/sites/default/files/bdl_image/header-T-OH.png\" title=\"Title text\" width=\"45%\"  />\n",
    "\n",
    "En algunos problemas como este, el comportamiento a detectar puede ser asociado a una anomalía (*outlier*) del comportamiento normal de los usuarios en las redes sociales. Esto es una de las causas de la dificultad del problema, puesto que es **altamente desbalanceado**, donde aproximadamente un 10% de los *tweets* corresponden a acoso (*harassment*).\n",
    "\n",
    "Los datos trabajados corresponderan a *tweets* etiquetados como *harassment* (con valor 1) o no (con valor 0) -- la tarea a detectar--. Además si desea utilizar, se incluye la información del tipo de *harassment* en el conjunto de entrenamiento como atributos extras. El conjunto de pruebas solo contiene los *tweets* a ser etiquetados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tweet_content\n",
      "harassment               \n",
      "0                    5154\n",
      "1                     549\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "#Test Set\n",
    "df_test = pd.read_csv(\"tarea3-ml/Test_input.csv\", encoding='ISO-8859-1')\n",
    "df_test_content = df_test.tweet_content\n",
    "y_test = pd.read_csv('tarea3-ml/sample_submission.csv')\n",
    "y_test = y_test.harassment\n",
    "#Train Set\n",
    "df_train = pd.read_csv(\"tarea3-ml/Train_data.csv\", encoding='ISO-8859-1')\n",
    "df_train = df_train.drop(columns=['id','IndirectH', 'PhysicalH', 'SexualH'])\n",
    "df_train_content = df_train.tweet_content\n",
    "labels_train = df_train.harassment\n",
    "\n",
    "tweet_by_harassment = df_train.groupby(['harassment']).count()\n",
    "print(tweet_by_harassment)\n",
    "plt.bar(['0','1'], tweet_by_harassment['tweet_content'], 0.35)\n",
    "plt.xlabel('Etiquetas')\n",
    "plt.ylabel('Nº Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que hay un desequilibrio entre las clases muy grande 5154 tweets sin acoso vs 549, por lo que habra que tener cuidado con el comportamiento de predicción de la clase 1.\n",
    "\n",
    "Es por esto que tambien se aplicaran técnicas para el manejo de datos desbalanceados, como lo son:\n",
    "* Eliminar muestras de la clase mayoritaria (**subsampling**): Tiene un punto en contra en que podriamos prescindir de muestras importantes.\n",
    "* Muestras Artificiales (**oversampling**): Intentar crear muestras sinteticas de la clase minoritaria tiene el punto encontra en que podemos cambiar la distribución natural de la clase y por tanto incurrir en altos errores de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "\n",
    "def base_word(word):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    return wordlemmatizer.lemmatize(word) \n",
    "def word_extractor(text_input):\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text_input) #substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ base_word(word.lower()) for word in word_tokenize(text) ]\n",
    "    \n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords: #and len(word)>3:  #delete stopwords and emogis\n",
    "            words+=\" \"+word\n",
    "    if len(words) == 0:\n",
    "        print(text_input)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train = [word_extractor(tweet) for tweet in df_train_content]\n",
    "content_test = [word_extractor(tweet) for tweet in df_test_content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación Term Frecuency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5703, 14863)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer= CountVectorizer(ngram_range=(1, 1), binary=False)\n",
    "vectorizer.fit(content_train)\n",
    "\n",
    "features_train = vectorizer.transform(content_train)\n",
    "features_test = vectorizer.transform(content_test)\n",
    "\n",
    "features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que se cuenta con un vocabulario de 14863 palabras, y por tanto tambien se tendra una alta dimensionalidad del problema. Es muy probable que los árboles de decisiones no funcionen con esta representación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.96      0.66       899\n",
      "           1       0.44      0.03      0.06       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.47      0.50      0.36      1785\n",
      "weighted avg       0.47      0.50      0.36      1785\n",
      "\n",
      "Acc: 0.4997198879551821\n",
      "f1-score: 0.05702217529039071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from  sklearn.metrics import f1_score\n",
    "from imblearn.ensemble import BalancedBaggingClassifier # doctest: +NORMALIZE_WHITESPACE\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Regresion Logistica\n",
    "model= LogisticRegression()\n",
    "model.set_params(penalty='l2',C=1)\n",
    "model.fit(features_train,labels_train)\n",
    "y_pred = model.predict(features_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(features_test, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.77      0.61       899\n",
      "           1       0.51      0.24      0.32       886\n",
      "\n",
      "    accuracy                           0.51      1785\n",
      "   macro avg       0.51      0.50      0.47      1785\n",
      "weighted avg       0.51      0.51      0.47      1785\n",
      "\n",
      "Acc: 0.5064425770308123\n",
      "f1-score: 0.3217859892224788\n"
     ]
    }
   ],
   "source": [
    "#Bagging Con Árbol de decision\n",
    "model = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=20)\n",
    "model.fit(features_train, labels_train)\n",
    "y_pred = model.predict(features_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(features_test, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.83      0.62       899\n",
      "           1       0.48      0.16      0.24       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.49      0.49      0.43      1785\n",
      "weighted avg       0.49      0.50      0.43      1785\n",
      "\n",
      "Acc: 0.49635854341736696\n",
      "f1-score: 0.2361937128292268\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "underSampling = NearMiss(n_neighbors=10, version=2)\n",
    "X_train_res, y_train_res = underSampling.fit_sample(features_train, labels_train)\n",
    "\n",
    "#Regresion Logistica Con Bagging\n",
    "model = BaggingClassifier(base_estimator=LogisticRegression(penalty='l2',C=10), n_estimators=20, n_jobs=-1).fit(X_train_res, y_train_res)\n",
    "y_pred = model.predict(features_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(features_test, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling + Smote-Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.58      0.54       899\n",
      "           1       0.50      0.42      0.45       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.50      0.50      0.50      1785\n",
      "weighted avg       0.50      0.50      0.50      1785\n",
      "\n",
      "Acc: 0.5002801120448179\n",
      "f1-score: 0.45276073619631907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "os_us = SMOTETomek()\n",
    "X_train_smote, y_train_smote = os_us.fit_sample(features_train, labels_train)\n",
    "\n",
    "#Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10)#max_depth=10\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = rf_model.predict(features_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {rf_model.score(features_test, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.12      0.20       899\n",
      "           1       0.50      0.89      0.64       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.52      0.51      0.42      1785\n",
      "weighted avg       0.52      0.50      0.42      1785\n",
      "\n",
      "Acc: 0.503641456582633\n",
      "f1-score: 0.6410048622366289\n"
     ]
    }
   ],
   "source": [
    "#SVM + Bagging\n",
    "model = BaggingClassifier(base_estimator=SVC(kernel='linear', C=0.001), n_estimators=100, n_jobs=-1)\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = model.predict(features_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(features_test, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "EMBEDDING_DIM = 300\n",
    "GLOVE_FILE = f\"glove/glove.6B.{EMBEDDING_DIM}d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE, encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_columns(sentences_vector):\n",
    "    c = 0\n",
    "    list_aux = []\n",
    "    matrix_n = np.empty((0,0))\n",
    "    for row in sentences_vector:\n",
    "        matrix = np.empty((0,0))\n",
    "#         print(\"2***********************\")\n",
    "        for word in row.split():\n",
    "#             print('***************************************************************************')\n",
    "            embd = embeddings_index.get(word)\n",
    "#             if embd is not None:\n",
    "#                 print(f'embd:{embd}')\n",
    "            if embd is None:\n",
    "                embd = np.zeros(300)\n",
    "                matrix = embd[np.newaxis,:] if matrix.shape == (0,0) else np.concatenate((matrix, embd[np.newaxis,:]), axis = 0)       \n",
    "            else:\n",
    "                matrix = embd[np.newaxis,:] if matrix.shape == (0,0) else np.concatenate((matrix,embd[np.newaxis,:]), axis = 0)\n",
    "#         print(f'matrix: {matrix} fin matrix')\n",
    "#         print(f'row: {rows}')\n",
    "        word_caracs = 0\n",
    "        if len(row) == 0:\n",
    "            word_caracs = np.zeros(300)\n",
    "        else:\n",
    "            word_caracs = matrix.mean(0)\n",
    "        matrix_n = word_caracs[np.newaxis,:] if matrix_n.shape == (0,0) else np.concatenate((matrix_n, word_caracs[np.newaxis,:]),\n",
    "                                                                                            axis = 0)\n",
    "        c+=1     \n",
    "    new_df = pd.DataFrame(list_aux)\n",
    "    return matrix_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_feature = generate_columns(content_train)\n",
    "Test_feature = generate_columns(content_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "underSampling = NearMiss(n_neighbors=3, version=2)\n",
    "X_train_res, y_train_res = underSampling.fit_sample(Train_feature, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.17      0.25       899\n",
      "           1       0.50      0.83      0.62       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.50      0.50      0.44      1785\n",
      "weighted avg       0.50      0.50      0.43      1785\n",
      "\n",
      "Acc: 0.4969187675070028\n",
      "f1-score: 0.6217354675652907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "## Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=20, max_depth=10)#max_depth=10\n",
    "rf_model.fit(X_train_res, y_train_res)\n",
    "y_pred = rf_model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {rf_model.score(Test_feature, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.42      0.46       899\n",
      "           1       0.49      0.58      0.53       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.50      0.50      0.49      1785\n",
      "weighted avg       0.50      0.50      0.49      1785\n",
      "\n",
      "Acc: 0.49635854341736696\n",
      "f1-score: 0.5315268368942158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "#Regresion Logistica + Bagging\n",
    "model = BaggingClassifier(base_estimator=LogisticRegression(penalty='l2',C=0.9), n_estimators=100, n_jobs=-1).fit(X_train_res, y_train_res)\n",
    "y_pred = model.predict(Test_feature)\n",
    "f1_score(y_test, y_pred, average='binary')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(Test_feature, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "termino\n"
     ]
    }
   ],
   "source": [
    "df_aux = pd.DataFrame()\n",
    "df_aux[\"id\"] = np.arange(1, 1+y_pred.shape[0])\n",
    "df_aux[\"harassment\"] = y_pred.astype('int')\n",
    "df_aux.to_csv(\"test_estimation6.csv\", index=False)\n",
    "print('termino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.47      0.49       899\n",
      "           1       0.49      0.52      0.51       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.50      0.50      0.50      1785\n",
      "weighted avg       0.50      0.50      0.50      1785\n",
      "\n",
      "Acc: 0.6747325968788357\n",
      "f1-score: 0.5087719298245613\n"
     ]
    }
   ],
   "source": [
    "#Ensamble de Modelos con Balanceo\n",
    "model = BalancedBaggingClassifier(base_estimator=SVC(kernel='linear'), n_estimators=10)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "y_pred = model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(Train_feature, labels_train)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.40      0.45       899\n",
      "           1       0.49      0.59      0.54       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.50      0.50      0.49      1785\n",
      "weighted avg       0.50      0.50      0.49      1785\n",
      "\n",
      "Acc: 0.6484306505348062\n",
      "f1-score: 0.5362840967575914\n"
     ]
    }
   ],
   "source": [
    "#Regresion Logistica Sola\n",
    "model= LogisticRegression()\n",
    "model.set_params(penalty='l2', class_weight=\"balanced\")\n",
    "model.fit(X_train_res,y_train_res)\n",
    "\n",
    "y_pred = model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(Train_feature, labels_train)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.40      0.45       899\n",
      "           1       0.49      0.59      0.54       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.50      0.50      0.49      1785\n",
      "weighted avg       0.50      0.50      0.49      1785\n",
      "\n",
      "Acc: 0.49523809523809526\n",
      "f1-score: 0.5362840967575914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator =LogisticRegression(penalty='l2',C=10), n_estimators=100, random_state=0)\n",
    "clf.fit(X_train_res, y_train_res)\n",
    "\n",
    "y_pred = model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {clf.score(Test_feature, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10308, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "overSampling =  RandomOverSampler()\n",
    "X_train_os, y_train_os = overSampling.fit_sample(Train_feature, labels_train)\n",
    "X_train_os.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.99      0.67       899\n",
      "           1       0.65      0.01      0.02       886\n",
      "\n",
      "    accuracy                           0.51      1785\n",
      "   macro avg       0.58      0.50      0.35      1785\n",
      "weighted avg       0.58      0.51      0.35      1785\n",
      "\n",
      "Acc: 0.5064425770308123\n",
      "f1-score: 0.024363233665559245\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=20)#max_depth=10\n",
    "rf_model.fit(X_train_os, y_train_os)\n",
    "y_pred = rf_model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {rf_model.score(Test_feature, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.73      0.59       899\n",
      "           1       0.50      0.28      0.35       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.50      0.50      0.47      1785\n",
      "weighted avg       0.50      0.50      0.48      1785\n",
      "\n",
      "Acc: 0.5019607843137255\n",
      "f1-score: 0.3543936092955701\n"
     ]
    }
   ],
   "source": [
    "#Regresion Logistica + Bagging\n",
    "model = BaggingClassifier(base_estimator=LogisticRegression(penalty='l2',C=0.0001), n_estimators=20, n_jobs=-1)\n",
    "model.fit(X_train_os, y_train_os)\n",
    "y_pred = model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(Test_feature, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.71      0.59       899\n",
      "           1       0.49      0.28      0.35       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.49      0.49      0.47      1785\n",
      "weighted avg       0.49      0.50      0.47      1785\n",
      "\n",
      "Acc: 0.8532351394003156\n",
      "f1-score: 0.35319454414931806\n"
     ]
    }
   ],
   "source": [
    "#SVM + Bagging\n",
    "model = BaggingClassifier(base_estimator=SVC(kernel='linear'), n_estimators=10)\n",
    "model.fit(X_train_os, y_train_os)\n",
    "y_pred = model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(Train_feature, labels_train)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.72      0.59       899\n",
      "           1       0.50      0.28      0.36       886\n",
      "\n",
      "    accuracy                           0.50      1785\n",
      "   macro avg       0.50      0.50      0.48      1785\n",
      "weighted avg       0.50      0.50      0.48      1785\n",
      "\n",
      "Acc: 0.8337716991057338\n",
      "f1-score: 0.3571945046999277\n"
     ]
    }
   ],
   "source": [
    "#Regresion Logistica\n",
    "model= LogisticRegression()\n",
    "model.set_params(penalty='l2', C=0.01)\n",
    "model.fit(X_train_os,y_train_os)\n",
    "\n",
    "y_pred = model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(Train_feature, labels_train)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling + Smote-Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.74      0.60       899\n",
      "           1       0.48      0.25      0.33       886\n",
      "\n",
      "    accuracy                           0.49      1785\n",
      "   macro avg       0.49      0.49      0.46      1785\n",
      "weighted avg       0.49      0.49      0.46      1785\n",
      "\n",
      "Acc: 0.8183412239172365\n",
      "f1-score: 0.32786885245901637\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek # doctest: +NORMALIZE_WHITESPACE\n",
    "\n",
    "os_us = SMOTETomek()\n",
    "X_train_smote, y_train_smote = os_us.fit_sample(Train_feature, labels_train)\n",
    "\n",
    "#Regresion Logistica + Bagging\n",
    "model = BaggingClassifier(base_estimator=LogisticRegression(penalty='l2', C=0.001), n_estimators=10)\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(Train_feature, labels_train)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.76      0.61       899\n",
      "           1       0.50      0.25      0.34       886\n",
      "\n",
      "    accuracy                           0.51      1785\n",
      "   macro avg       0.51      0.50      0.47      1785\n",
      "weighted avg       0.51      0.51      0.47      1785\n",
      "\n",
      "Acc: 0.8628791863931264\n",
      "f1-score: 0.3358433734939759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\castillo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#Regresion Logistica\n",
    "model= LogisticRegression()\n",
    "model.set_params(penalty='l2', C=1)\n",
    "model.fit(X_train_smote,y_train_smote)\n",
    "\n",
    "y_pred = model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {model.score(Train_feature, labels_train)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.97      0.67       899\n",
      "           1       0.59      0.04      0.08       886\n",
      "\n",
      "    accuracy                           0.51      1785\n",
      "   macro avg       0.55      0.51      0.37      1785\n",
      "weighted avg       0.55      0.51      0.38      1785\n",
      "\n",
      "Acc: 0.5103641456582633\n",
      "f1-score: 0.0819327731092437\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=20)#max_depth=10\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "y_pred = rf_model.predict(Test_feature)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'Acc: {rf_model.score(Test_feature, y_test)}')\n",
    "print(f'f1-score: {f1_score(y_test, y_pred, average=\"binary\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
